{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Simple Local AI Voice Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "\n",
    "1. Be able to control the voice assistant from voice\n",
    "2. BE able to talk to it as well as perform commands/actions \n",
    "3. Tasks\n",
    "   1. Create tasks in some task backlog db\n",
    "   2. Create, read, edit and delete files locally\n",
    "   3. Send emails\n",
    "   4. Fully local setup (audio transcription and the AI should be fully local)\n",
    "   5. Answer questions about personal notes and knowledge management stuff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "c:\\Users\\carli\\miniconda3\\envs\\ace\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:604: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
      "Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This is the audio test file.\n"
     ]
    }
   ],
   "source": [
    "# 1 - Voice Control \n",
    "# We'll need an audion transcription model to convert audio to text \n",
    "# We'll use whisper turbo 3\n",
    "# source: https://huggingface.co/openai/whisper-large-v3-turbo\n",
    "# pip install --upgrade pip\n",
    "# pip install --upgrade transformers datasets[audio] accelerate\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3-turbo\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "result = pipe(\"audio-testfile.mp3\")\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "model \"llama3.3\" not found, try pulling it first (status code: 404)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResponseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 2 - Interaction using LLM\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# We'll use llama 3.2 with Ollama https://ollama.com/\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# pip install ollama\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# https://github.com/ollama/ollama-python\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mollama\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m response = \u001b[43mollama\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mllama3.3\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m  \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mWhy is the sky blue?\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m  \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(response[\u001b[33m'\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carli\\miniconda3\\envs\\ace\\Lib\\site-packages\\ollama\\_client.py:342\u001b[39m, in \u001b[36mClient.chat\u001b[39m\u001b[34m(self, model, messages, tools, stream, think, format, options, keep_alive)\u001b[39m\n\u001b[32m    297\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchat\u001b[39m(\n\u001b[32m    298\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    299\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    307\u001b[39m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    308\u001b[39m ) -> Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[32m    309\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    310\u001b[39m \u001b[33;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[32m    311\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    340\u001b[39m \u001b[33;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[32m    341\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m    \u001b[49m\u001b[43mChatResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/api/chat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_copy_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_copy_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m      \u001b[49m\u001b[43mthink\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthink\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carli\\miniconda3\\envs\\ace\\Lib\\site-packages\\ollama\\_client.py:180\u001b[39m, in \u001b[36mClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    176\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**part)\n\u001b[32m    178\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carli\\miniconda3\\envs\\ace\\Lib\\site-packages\\ollama\\_client.py:124\u001b[39m, in \u001b[36mClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    122\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m r\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.ConnectError:\n\u001b[32m    126\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(CONNECTION_ERROR_MESSAGE) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mResponseError\u001b[39m: model \"llama3.3\" not found, try pulling it first (status code: 404)"
     ]
    }
   ],
   "source": [
    "# 2 - Interaction using LLM\n",
    "# We'll use llama 3.2 with Ollama https://ollama.com/\n",
    "# pip install ollama\n",
    "# https://github.com/ollama/ollama-python\n",
    "\n",
    "import ollama\n",
    "response = ollama.chat(model='llama3.2', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It\\'s challenging to pinpoint a single country with the \"best\" weather, as opinions on ideal weather conditions vary greatly. However, some countries are often considered to have pleasant and consistent climates.\\n\\nAccording to various surveys, reviews, and climate data, countries like:\\n\\n1. **Hawaii (USA)**: Known for its tropical climate with warm temperatures (70-85°F/21-30°C) and low humidity throughout the year.\\n2. **Barbados**: A small island nation in the Caribbean with a subtropical climate, featuring mild temperatures (75-82°F/24-28°C) and plenty of sunshine (over 300 days per year).\\n3. **Maldives**: An island nation in the Indian Ocean with a tropical monsoon climate, characterized by warm temperatures (84-91°F/29-33°C) and high humidity.\\n4. **Spain (Mediterranean coast)**: The southern region of Spain, particularly the Costa del Sol, enjoys a mild Mediterranean climate with pleasant temperatures (64-77°F/18-25°C) during the summer months.\\n\\nHowever, if I had to pick one country that\\'s often considered to have the best weather, it would be:\\n\\n**Bermuda**: Located in the North Atlantic Ocean, Bermuda boasts an exceptional subtropical climate. The island experiences mild winters (50-64°F/10-18°C) and warm summers (75-82°F/24-28°C), with average temperatures ranging from 67-73°F (19-23°C). The humidity is relatively low, and the sun shines brightly for over 340 days per year.\\n\\nKeep in mind that \"best\" weather is subjective, and personal preferences may vary. Ultimately, the best country for you will depend on your individual climate preferences!'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_response(prompt):\n",
    "    response = ollama.chat(model='llama3.2', \n",
    "                           messages=[{'role': 'user', 'content': prompt}])\n",
    "    return response['message']['content']\n",
    "\n",
    "get_response(\"What is the country known for having the best weather in the world?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording finished.\n",
      "Audio saved as prompt.wav\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "def record_audio(filename=\"prompt.mp3\", duration=5, sample_rate=44100, channels=2, chunk=1024):\n",
    "    \"\"\"\n",
    "    Record audio from the microphone and save it to a file.\n",
    "    \n",
    "    :param filename: Name of the output file (default: \"prompt.mp3\")\n",
    "    :param duration: Duration of the recording in seconds (default: 5)\n",
    "    :param sample_rate: Sample rate of the recording (default: 44100 Hz)\n",
    "    :param channels: Number of audio channels (default: 2 for stereo)\n",
    "    :param chunk: Number of frames per buffer (default: 1024)\n",
    "    \"\"\"\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = p.open(format=pyaudio.paInt16,\n",
    "                    channels=1,\n",
    "                    rate=sample_rate,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=chunk)\n",
    "\n",
    "    print(\"Recording...\")\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    for i in range(0, int(sample_rate / chunk * duration)):\n",
    "        data = stream.read(chunk)\n",
    "        frames.append(data)\n",
    "\n",
    "    print(\"Recording finished.\")\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    # Save the recorded data as a WAV file\n",
    "    wf = wave.open(filename.replace('.mp3', '.wav'), 'wb')\n",
    "    wf.setnchannels(channels)\n",
    "    wf.setsampwidth(p.get_sample_size(pyaudio.paInt16))\n",
    "    wf.setframerate(sample_rate)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()\n",
    "\n",
    "    print(f\"Audio saved as {filename.replace('.mp3', '.wav')}\")\n",
    "\n",
    "# Example usage:            \n",
    "record_audio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Why is the sky blue?'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def transcribe(audio_filepath):\n",
    "    result = pipe(audio_filepath)\n",
    "    return result[\"text\"]\n",
    "\n",
    "transcribe(\"./prompt.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording finished.\n",
      "Audio saved as prompt.wav\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I\\'d be happy to try. However, I need more information from you about the type of song and lyrics you\\'re looking for.\\n\\nHere\\'s a sample verse with the phrase \"Cutting knowledge date\" written as a lyric:\\n\\n\"In the digital age, we\\'re living in time\\nWhere knowledge is power, but it\\'s hard to define\\nA cutting knowledge date, that\\'s what they say\\nBut information is fleeting, and it\\'s here to stay\"\\n\\nPlease let me know if you\\'d like me to modify this verse or write an entirely new song based on the phrase \"Cutting Knowledge Date\". What genre of music are you looking for (e.g. pop, rock, hip-hop)?'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_audio()\n",
    "prompt = transcribe(\"./prompt.wav\")\n",
    "get_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks**\n",
    "   1. Create tasks in some task backlog db\n",
    "   2. Create, read, edit and delete files locally\n",
    "   3. Send emails\n",
    "   4. Fully local setup (audio transcription and the AI should be fully local)\n",
    "   5. Answer questions about personal notes and knowledge management stuff \n",
    "\n",
    "   6. GPS Directions\n",
    "   7. Local trivia\n",
    "   8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>status</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>completed_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [task, status, creation_date, completed_date]\n",
       "Index: []"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the task db first before writing the tools for the model\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Create an empty DataFrame for the tasks database\n",
    "tasks_df = pd.DataFrame(columns=['task', 'status', 'creation_date', 'completed_date'])\n",
    "\n",
    "tasks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool for adding a task\n",
    "def add_task(task_description):\n",
    "    \"\"\"\n",
    "    Add a task to the tasks database.\n",
    "    \"\"\"\n",
    "    new_task = pd.DataFrame({\n",
    "        'task': [task_description],\n",
    "        'status': ['Not Started'],\n",
    "        'creation_date': [datetime.now().strftime('%Y-%m-%d %H:%M:%S')],\n",
    "        'completed_date': [None]\n",
    "    })\n",
    "    global tasks_df\n",
    "    tasks_df = pd.concat([tasks_df, new_task], ignore_index=True)\n",
    "    \n",
    "    return tasks_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool Calling\n",
    "\n",
    "Tool calling is about giving LLMs the ability to perform actions.\n",
    "\n",
    "```\n",
    "{\n",
    "    'type': 'function',\n",
    "    'function': {\n",
    "        'name': 'create_file',\n",
    "        'description': 'Create a new file with given content',\n",
    "        'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "                'filename': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'The name of the file to create',\n",
    "                },\n",
    "                'content': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'The content to write to the file',\n",
    "                },\n",
    "            },\n",
    "            'required': ['filename', 'content'],\n",
    "        },\n",
    "    },\n",
    "},\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_add_tasks_to_db = {\n",
    "    'type': 'function',\n",
    "    'function': {\n",
    "        'name': 'add_task',\n",
    "        'description': 'Add a task to the tasks database',\n",
    "        'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "                'task_description': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'The description of the task to add',\n",
    "                },\n",
    "            },\n",
    "            'required': ['task_description'],\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tasks in a backlog task db\n",
    "def get_response_with_tools(prompt):\n",
    "    response = ollama.chat(model='llama3.2', \n",
    "                           messages=[{'role': 'user', 'content': prompt}],\n",
    "                           tools=[tool_add_tasks_to_db])\n",
    "    # Process tool calls if present\n",
    "    if 'tool_calls' in response['message']:\n",
    "        for tool_call in response['message']['tool_calls']:\n",
    "            if tool_call['function']['name'] == 'add_task':\n",
    "                task_description = tool_call['function']['arguments']['task_description']\n",
    "                add_task(task_description)\n",
    "                print(f\"Task added: {task_description}\")\n",
    "    else:\n",
    "        return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task added: Create a local voice AI assistant\n"
     ]
    }
   ],
   "source": [
    "get_response_with_tools(\"Create a task to create a local voice AI assistant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>status</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>completed_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Create a local voice AI assistant</td>\n",
       "      <td>Not Started</td>\n",
       "      <td>2025-07-16 14:16:46</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                task       status        creation_date  \\\n",
       "0  Create a local voice AI assistant  Not Started  2025-07-16 14:16:46   \n",
       "\n",
       "  completed_date  \n",
       "0           None  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_create_file = {\n",
    "            'type': 'function',\n",
    "            'function': {\n",
    "                'name': 'create_file',\n",
    "                'description': 'Create a new file with given content',\n",
    "                'parameters': {\n",
    "                    'type': 'object',\n",
    "                    'properties': {\n",
    "                        'filename': {\n",
    "                            'type': 'string',\n",
    "                            'description': 'The name of the file to create',\n",
    "                        },\n",
    "                        'content': {\n",
    "                            'type': 'string',\n",
    "                            'description': 'The content to write to the file',\n",
    "                        },\n",
    "                    },\n",
    "                    'required': ['filename', 'content'],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "tool_read_file = {\n",
    "            'type': 'function',\n",
    "            'function': {\n",
    "                'name': 'read_file',\n",
    "                'description': 'Read the content of a file',\n",
    "                'parameters': {\n",
    "                    'type': 'object',\n",
    "                    'properties': {\n",
    "                        'filename': {\n",
    "                            'type': 'string',\n",
    "                            'description': 'The name of the file to read',\n",
    "                        },\n",
    "                    },\n",
    "                    'required': ['filename'],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "tool_delete_file = {\n",
    "            'type': 'function',\n",
    "            'function': {\n",
    "                'name': 'delete_file',\n",
    "                'description': 'Delete a file',\n",
    "                'parameters': {\n",
    "                    'type': 'object',\n",
    "                    'properties': {\n",
    "                        'filename': {\n",
    "                            'type': 'string',\n",
    "                            'description': 'The name of the file to delete',\n",
    "                        },\n",
    "                    },\n",
    "                    'required': ['filename'],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "\n",
    "tool_edit_file = {\n",
    "            'type': 'function', \n",
    "            'function': {\n",
    "                'name': 'edit_file',\n",
    "                'description': 'Edit the content of a file',\n",
    "                'parameters': {\n",
    "                    'type': 'object',\n",
    "                    'properties': {\n",
    "                        'filename': {\n",
    "                            'type': 'string',\n",
    "                            'description': 'The name of the file to edit',\n",
    "                        },\n",
    "                        'content': {\n",
    "                            'type': 'string',\n",
    "                            'description': 'The content to write to the file',\n",
    "                        },\n",
    "                    },\n",
    "                    'required': ['filename', 'content'],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "tools = [tool_create_file, tool_read_file, tool_delete_file, tool_add_tasks_to_db]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Writing functions to create, read, edit and delete files\n",
    "\n",
    "def create_file(filename, content):\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(content)\n",
    "    return f\"File {filename} created successfully\"\n",
    "\n",
    "def read_file(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "def edit_file(filename, content):\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(content)\n",
    "    return f\"File {filename} edited successfully\"\n",
    "\n",
    "def delete_file(filename):\n",
    "    os.remove(filename)\n",
    "    return f\"File {filename} deleted successfully\"\n",
    "\n",
    "# Creating tasks in a backlog task db\n",
    "def get_response_with_tools(prompt):\n",
    "    response = ollama.chat(model='llama3.2', \n",
    "                           messages=[{'role': 'user', 'content': prompt}],\n",
    "                           tools=tools)\n",
    "    # Process tool calls if present\n",
    "    if 'tool_calls' in response['message']:\n",
    "        for tool_call in response['message']['tool_calls']:\n",
    "            if tool_call['function']['name'] == 'add_task':\n",
    "                task_description = tool_call['function']['arguments']['task_description']\n",
    "                add_task(task_description)\n",
    "                print(f\"Task added: {task_description}\")\n",
    "            elif tool_call['function']['name'] == 'create_file':\n",
    "                print(\"Creating file...\")\n",
    "                filename = tool_call['function']['arguments']['filename']\n",
    "                content = tool_call['function']['arguments']['content']\n",
    "                create_file(filename, content)\n",
    "                print(f\"File created: {filename}\")\n",
    "            elif tool_call['function']['name'] == 'read_file':\n",
    "                print(\"Reading file...\")\n",
    "                filename = tool_call['function']['arguments']['filename']\n",
    "                content = read_file(filename)\n",
    "                print(f\"File content: {content}\")\n",
    "            elif tool_call['function']['name'] == 'delete_file':\n",
    "                print(\"Deleting file...\")\n",
    "                filename = tool_call['function']['arguments']['filename']\n",
    "                delete_file(filename)\n",
    "                print(f\"File deleted: {filename}\")\n",
    "    else:\n",
    "        return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating file...\n",
      "File created: test.txt\n"
     ]
    }
   ],
   "source": [
    "get_response_with_tools(\"Create a file called 'test.txt' with the content 'Hello, world!'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>status</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>completed_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Create a local voice AI assistant</td>\n",
       "      <td>Not Started</td>\n",
       "      <td>2025-07-16 14:16:46</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                task       status        creation_date  \\\n",
       "0  Create a local voice AI assistant  Not Started  2025-07-16 14:16:46   \n",
       "\n",
       "  completed_date  \n",
       "0           None  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording finished.\n",
      "Audio saved as prompt.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carli\\miniconda3\\envs\\ace\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:604: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating file...\n",
      "File created: experience.txt\n"
     ]
    }
   ],
   "source": [
    "record_audio(duration=5)\n",
    "prompt = transcribe(\"./prompt.wav\")\n",
    "get_response_with_tools(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks saved to tasks.csv\n"
     ]
    }
   ],
   "source": [
    "# Save tasks_df to CSV file\n",
    "tasks_df.to_csv('tasks.csv', index=False)\n",
    "print(\"Tasks saved to tasks.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
